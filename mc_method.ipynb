{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from Env import GridWorld\n",
    "\n",
    "DISCOUNT_FACTOR = 0.9\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        \n",
    "    def draw_one_episode(self, policy, max_length=1000):\n",
    "        episode = []\n",
    "        state = self.env.reset()\n",
    "        for t in range(max_length):\n",
    "            action = policy(state)\n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "        return episode\n",
    "    \n",
    "    def accumulate_state_reward_from_episode(self, s, episode):\n",
    "        first_occur_idx = next(i for i, x in enumerate(episode) if x[0] == s)\n",
    "        reward = sum([x[2] * (DISCOUNT_FACTOR ** i)\n",
    "                      for i, x in enumerate(episode[first_occur_idx:])])\n",
    "        return reward\n",
    "    \n",
    "    def accumulate_state_action_reward_from_episode(self, s, a, episode):\n",
    "        first_occur_idx = next(i for i, x in enumerate(episode)\n",
    "                               if x[0] == s and x[1] == a)\n",
    "        reward = sum([x[2] * (DISCOUNT_FACTOR ** i)\n",
    "                      for i, x in enumerate(episode[first_occur_idx:])])\n",
    "        return reward\n",
    "\n",
    "    def estimate_state_value_first_visit(self, policy, num_episodes=100):\n",
    "        state2reward = np.zeros(self.env.nS)\n",
    "        state2count = np.zeros(self.env.nS)\n",
    "        V = np.zeros(self.env.nS)\n",
    "        \n",
    "        for ith in range(1, num_episodes + 1):\n",
    "            if ith % 10 == 0:\n",
    "                print(\"\\rEpisode {}/{}\".format(ith, num_episodes, ))\n",
    "                print(np.reshape(V, env.shape))\n",
    "            \n",
    "            episode = self.draw_one_episode(policy)\n",
    "            \n",
    "            states_in_episode = set([x[0] for x in episode])\n",
    "            for s in states_in_episode:\n",
    "                reward = self.accumulate_state_reward_from_episode(s, episode)\n",
    "                state2reward[s] += reward\n",
    "                state2count[s] += 1.0\n",
    "                V[s] = state2reward[s] / state2count[s]\n",
    "        return V\n",
    "    \n",
    "    def estimate_state_action_value_first_visit(self, policy, num_episodes=100000):\n",
    "        sa2reward = np.zeros((self.env.nS, self.env.nA))\n",
    "        sa2count = np.zeros((self.env.nS, self.env.nA))\n",
    "        Q = np.zeros((self.env.nS, self.env.nA))\n",
    "        \n",
    "        for ith in range(1, num_episodes + 1):\n",
    "            if ith % 1000 == 0:\n",
    "                print(\"\\rEpisode {}/{}\".format(ith, num_episodes, ))\n",
    "                print(np.reshape(Q, (self.env.nS, self.env.nA)))\n",
    "            \n",
    "            episode = self.draw_one_episode(policy)\n",
    "            \n",
    "            sa_in_episode = set([(x[0], x[1]) for x in episode])\n",
    "            for s, a in sa_in_episode:\n",
    "                reward = self.accumulate_state_action_reward_from_episode(s, a, episode)\n",
    "                sa2reward[s][a] += reward\n",
    "                sa2count[s][a] += 1.0\n",
    "                Q[s][a] = sa2reward[s][a] / sa2count[s][a]\n",
    "        return Q\n",
    "    \n",
    "#     def estimate_state_action_value_every_visit_importance_sampling(self, policy, num_episodes):\n",
    "#         Q = defaultdict(float)\n",
    "#         sa2count = defaultdict(float)\n",
    "        \n",
    "#         for ith in range(1, num_episodes + 1):\n",
    "#             if ith % 1000 == 0:\n",
    "#                 print(\"\\rEpisode {}/{}\".format(ith, num_episodes, ))\n",
    "            \n",
    "#             episode = self.draw_one_episode(policy)\n",
    "            \n",
    "#             G = 0.0\n",
    "#             W = 1.0\n",
    "#             for state, action, reward in episode[::-1]:\n",
    "#                 G = DISCOUNT_FACTOR * G + reward\n",
    "#                 sa2count[(state, action)] += W\n",
    "#                 Q[(state, aciton)] += (W / sa2count[(state, action)]) * (G - Q[(state, action)])\n",
    "#                 if action != np.argmax(target_policy(state)):\n",
    "#                     break\n",
    "#                 W = W * 1.0 / behavior_policy(state)[action]\n",
    "#             sa_in_episode = set([(x[0], x[1]) for x in episode])\n",
    "#             for s, a in sa_in_episode:\n",
    "#                 reward = self.accumulate_state_action_reward_from_episode(sa, episode)\n",
    "#                 sa2reward[s] += reward\n",
    "#                 sa2count[s] += 1.0\n",
    "        \n",
    "#         for sa in sa2reward:\n",
    "#             Q[sa] = sa2reward[s] / sa2count[s]\n",
    "\n",
    "#         return Q, target_policy\n",
    "\n",
    "\n",
    "# estimate state value\n",
    "env = GridWorld(wind_prob=0.2)\n",
    "agent = Agent(env)\n",
    "policy = lambda state: 1\n",
    "V = agent.estimate_state_value_first_visit(policy)\n",
    "\n",
    "# estimate state action value\n",
    "# env = GridWorld(wind_prob=0.2)\n",
    "# agent = Agent(env)\n",
    "# policy = lambda state: np.random.choice(4)\n",
    "# Q = agent.estimate_state_action_value_first_visit(policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jupyter-python36]",
   "language": "python",
   "name": "conda-env-jupyter-python36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
