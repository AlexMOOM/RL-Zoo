{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Value Iteration: Round 0\n",
      "[[ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "\n",
      "Value Iteration: Round 1\n",
      "[[-1. -1. -1.]\n",
      " [-1. -1. -1.]\n",
      " [-1. -1.  0.]]\n",
      "\n",
      "Value Iteration: Round 2\n",
      "[[-2. -2. -2.]\n",
      " [-2. -2. -1.]\n",
      " [-2. -1.  0.]]\n",
      "\n",
      "Value Iteration: Round 3\n",
      "[[-3. -3. -2.]\n",
      " [-3. -2. -1.]\n",
      " [-2. -1.  0.]]\n",
      "\n",
      "Value Iteration: Round 4\n",
      "[[-4. -3. -2.]\n",
      " [-3. -2. -1.]\n",
      " [-2. -1.  0.]]\n",
      "\n",
      "Best Policy\n",
      "[['D' 'D' 'D']\n",
      " ['D' 'D' 'D']\n",
      " ['R' 'R' 'U']]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from Env import GridWorld\n",
    "\n",
    "\n",
    "DISCOUNT_FACTOR = 1\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.V = np.zeros(env.nS)\n",
    "\n",
    "    def next_best_action(self, s, V):\n",
    "        action_values = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            for prob, next_state, reward, done in env.P[s][a]:\n",
    "                action_values[a] += prob * (reward + DISCOUNT_FACTOR * V[next_state])\n",
    "        return np.argmax(action_values), np.max(action_values)\n",
    "\n",
    "    def optimize(self):\n",
    "        THETA = 0.0001\n",
    "        delta = float(\"inf\")\n",
    "        round_num = 0\n",
    "\n",
    "        while delta > THETA:\n",
    "            delta = 0\n",
    "            print(\"\\nValue Iteration: Round \" + str(round_num))\n",
    "            print(np.reshape(self.V, env.shape))\n",
    "            for s in range(env.nS):\n",
    "                best_action, best_action_value = self.next_best_action(s, self.V)\n",
    "                delta = max(delta, np.abs(best_action_value - self.V[s]))\n",
    "                self.V[s] = best_action_value\n",
    "            round_num += 1\n",
    "\n",
    "        policy = np.zeros(env.nS)\n",
    "        for s in range(env.nS):\n",
    "            best_action, best_action_value = self.next_best_action(s, self.V)\n",
    "            policy[s] = best_action\n",
    "\n",
    "        return policy\n",
    "\n",
    "\n",
    "env = GridWorld()\n",
    "agent = Agent(env)\n",
    "policy = agent.optimize()\n",
    "print(\"\\nBest Policy\")\n",
    "print(np.reshape([env.get_action_name(entry) for entry in policy], env.shape))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jupyter-python36]",
   "language": "python",
   "name": "conda-env-jupyter-python36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
