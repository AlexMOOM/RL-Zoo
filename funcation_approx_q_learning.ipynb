{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import itertools\n",
    "import numpy as np\n",
    "import sklearn.preprocessing\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "\n",
    "DISCOUNT_FACTOR = 0.9\n",
    "\n",
    "\n",
    "class Estimator:\n",
    "    def __init__(self, env):\n",
    "        self.models = []\n",
    "        for _ in range(env.action_space.n):\n",
    "            model = SGDRegressor()\n",
    "            model.partial_fit([env.reset()], [0])\n",
    "            self.models.append(model)\n",
    "    \n",
    "    def predict(self, s, a=None):\n",
    "        if a is None:\n",
    "            return [model.predict([s])[0] for model in self.models]\n",
    "        else:\n",
    "            return self.models[a].predict([s])[0]\n",
    "    \n",
    "    def update(self, s, a, y):\n",
    "        self.models[a].partial_fit([s], [y])\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "    \n",
    "    def policy(self, estimator, state, epsilon=0.01):\n",
    "        action_probs = np.ones(self.env.action_space.n, dtype=float) * epsilon / self.env.action_space.n\n",
    "        action_probs[np.argmax(estimator.predict(state))] += (1.0 - epsilon)\n",
    "        action = np.random.choice(self.env.action_space.n, p=action_probs)\n",
    "        return action\n",
    "    \n",
    "    def q_learning(self, estimator, num_episodes=1000, alpha=0.5):\n",
    "        round_num = []\n",
    "        for ith in range(1, num_episodes + 1):\n",
    "            if ith % 100 == 0:\n",
    "                print(\"\\rEpisode {}/{}.\".format(ith, num_episodes))\n",
    "                print(np.mean(round_num))\n",
    "                \n",
    "            state = env.reset()\n",
    "            \n",
    "            for t in itertools.count():\n",
    "                action = self.policy(estimator, state)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                \n",
    "                print (action, next_state)\n",
    "                               \n",
    "                q_values_next = estimator.predict(next_state)\n",
    "                td_target = reward + DISCOUNT_FACTOR * np.max(q_values_next)\n",
    "                \n",
    "                estimator.update(state, action, td_target)\n",
    "                \n",
    "                if done:\n",
    "                    round_num.append(t)\n",
    "                    break\n",
    "                \n",
    "                state = next_state\n",
    "                \n",
    "        return round_num\n",
    "\n",
    "\n",
    "\n",
    "# state = [position in [-1.2, -0.6], velocity in [-0.07, 0.07]]\n",
    "\n",
    "\n",
    "env = gym.envs.make(\"MountainCar-v0\")\n",
    "agent = Agent(env)\n",
    "estimator = Estimator(env)\n",
    "round_num = agent.q_learning(estimator)\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jupyter-python36]",
   "language": "python",
   "name": "conda-env-jupyter-python36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
