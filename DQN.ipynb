{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import itertools\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "env = gym.envs.make('Breakout-v0')\n",
    "VALID_ACTIONS = [0, 1, 2, 3] # 0 (NOOP), 1 (FIRE), 2 (LEFT), 3(RIGHT)\n",
    "\n",
    "\n",
    "class StateProcessor():\n",
    "    def __init__(self):\n",
    "        with tf.variable_scope(\"state_prosessor\"):\n",
    "            self.input_state = tf.placeholder(shape=[210, 160, 3], dtype=tf.uint8)\n",
    "            self.output = tf.image.rgb_to_grayscale(self.input_state)\n",
    "            self.output = tf.image.crop_to_bounding_box(self.output, 34, 0, 160, 160)\n",
    "            self.output = tf.image.resize_images(self.output, [84, 84],\n",
    "                                                 method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "            self.output = tf.squeeze(self.output)\n",
    "    \n",
    "    def process(self, sess, state):\n",
    "        return sess.run(self.output, {self.input_state: state})\n",
    "    \n",
    "\n",
    "class Estimator():\n",
    "    def __init__(self, scope=\"estimator\"):\n",
    "        self.scope = scope\n",
    "        with tf.variable_scope(scope):\n",
    "            self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        self.X_pl = tf.placeholder(shape=[None, 84, 84, 4], dtype=tf.uint8, name=\"X\")\n",
    "        self.y_pl = tf.placeholder(shape=[None], dtype=tf.float32, name=\"y\")\n",
    "        self.actions_pl = tf.placeholder(shape=[None], dtype=tf.int32, name=\"actions\")\n",
    "        \n",
    "        X = tf.to_float(self.X_pl) / 255.0\n",
    "        batch_size = tf.shape(self.X_pl)[0]\n",
    "        \n",
    "        conv1 = tf.contrib.layers.conv2d(\n",
    "            X, 32, 8, 4, activation_fn=tf.nn.relu)\n",
    "        conv2 = tf.contrib.layers.conv2d(\n",
    "            conv1, 64, 4, 2, activation_fn=tf.nn.relu)\n",
    "        conv3 = tf.contrib.layers.conv2d(\n",
    "            conv2, 64, 3, 1, activation_fn=tf.nn.relu)\n",
    "        \n",
    "        flattened = tf.contrib.layers.flatten(conv3)\n",
    "        fc1 = tf.contrib.layers.fully_connected(flattened, 512)\n",
    "        self.predictions = tf.contrib.layers.fully_connected(fc1, len(VALID_ACTIONS))\n",
    "        \n",
    "        gather_indices = tf.range(batch_size) * tf.shape(self.predictions)[1] + self.actions_pl\n",
    "        self.action_predictions = tf.gather(tf.reshape(self.predictions, [-1]), gather_indices)\n",
    "        \n",
    "        self.losses = tf.squared_difference(self.y_pl, self.action_predictions)\n",
    "        self.loss = tf.reduce_mean(self.losses)\n",
    "\n",
    "        # Optimizer Parameters from original paper\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6)\n",
    "        self.train_op = self.optimizer.minimize(self.loss, global_step=tf.contrib.framework.get_global_step())\n",
    "    \n",
    "    def predict(self, sess, s):\n",
    "        return sess.run(self.predictions, { self.X_pl: s })\n",
    "    \n",
    "    def update(self, sess, s, a, y):\n",
    "        feed_dict = { self.X_pl: s, self.y_pl: y, self.actions_pl: a }\n",
    "        global_step, _, loss = sess.run([tf.contrib.framework.get_global_step(), self.train_op, self.loss],\n",
    "                                        feed_dict)\n",
    "        return loss\n",
    "    \n",
    "class ModelParametersCopier():\n",
    "    def __init__(self, estimator1, estimator2):\n",
    "        e1_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator1.scope)]\n",
    "        e1_params = sorted(e1_params, key=lambda v: v.name)\n",
    "        e2_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator2.scope)]\n",
    "        e2_params = sorted(e2_params, key=lambda v: v.name)\n",
    "\n",
    "        self.update_ops = []\n",
    "        for e1_v, e2_v in zip(e1_params, e2_params):\n",
    "            op = e2_v.assign(e1_v)\n",
    "            self.update_ops.append(op)\n",
    "            \n",
    "    def make(self, sess):\n",
    "        sess.run(self.update_ops)\n",
    "        \n",
    "def make_epsilon_greedy_policy(estimator, nA):\n",
    "    def policy_fn(sess, observation, epsilon):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        q_values = estimator.predict(sess, np.expand_dims(observation, 0))[0]\n",
    "        best_action = np.argmax(q_values)\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn\n",
    "\n",
    "def deep_q_learning(sess,\n",
    "                    env,\n",
    "                    q_estimator,\n",
    "                    target_estimator,\n",
    "                    state_processor,\n",
    "                    num_episodes,\n",
    "                    replay_memory_size=500000,\n",
    "                    replay_memory_init_size=500,\n",
    "                    update_target_estimator_every=10000,\n",
    "                    discount_factor=0.99,\n",
    "                    epsilon_start=1.0,\n",
    "                    epsilon_end=0.1,\n",
    "                    epsilon_decay_steps=500000,\n",
    "                    batch_size=32,\n",
    "                    record_video_every=50):\n",
    "    \n",
    "    replay_memory = []\n",
    "    Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "    \n",
    "    estimator_copy = ModelParametersCopier(q_estimator, target_estimator)\n",
    "    total_t = sess.run(tf.contrib.framework.get_global_step())\n",
    "    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
    "    policy = make_epsilon_greedy_policy(q_estimator, len(VALID_ACTIONS))\n",
    "    \n",
    "    print(\"Populating replay memory...\")\n",
    "    state = env.reset()\n",
    "    state = state_processor.process(sess, state)\n",
    "    state = np.stack([state] * 4, axis=2)\n",
    "    for i in range(replay_memory_init_size):\n",
    "        if i % 100 == 0:\n",
    "            print(i)\n",
    "        action_probs = policy(sess, state, epsilons[min(total_t, epsilon_decay_steps - 1)])\n",
    "        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "        next_state, reward, done, _ = env.step(VALID_ACTIONS[action])\n",
    "        next_state = state_processor.process(sess, next_state)\n",
    "        next_state = np.append(state[:, :, 1:], np.expand_dims(next_state, 2), axis=2)\n",
    "        replay_memory.append(Transition(state, action, reward, next_state, done))\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            state = state_processor.process(sess, state)\n",
    "            state = np.stack([state] * 4, axis=2)\n",
    "        else:\n",
    "            state = next_state\n",
    "    \n",
    "    round_num = []\n",
    "    for i_episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        state = state_processor.process(sess, state)\n",
    "        state = np.stack([state] * 4, axis=2)\n",
    "        loss = None\n",
    "        \n",
    "        for t in itertools.count():\n",
    "            if t % 100 == 0:\n",
    "                print(\"round \" + str(t) + \" :\" + str(np.mean(round_num)))\n",
    "            \n",
    "            epsilon = epsilons[min(total_t, epsilon_decay_steps - 1)]\n",
    "            \n",
    "            action_probs = policy(sess, state, epsilon)\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            next_state, reward, done, _ = env.step(VALID_ACTIONS[action])\n",
    "            next_state = state_processor.process(sess, next_state)\n",
    "            next_state = np.append(state[:, :, 1:], np.expand_dims(next_state, 2), axis=2)\n",
    "            \n",
    "            if len(replay_memory) == replay_memory_size:\n",
    "                replay_memory.pop(0)\n",
    "            replay_memory.append(Transition(state, action, reward, next_state, done))\n",
    "            \n",
    "            samples = random.sample(replay_memory, batch_size)\n",
    "            states_batch, action_batch, reward_batch, next_states_batch, done_batch = map(np.array, zip(*samples))\n",
    "            \n",
    "            q_values_next = target_estimator.predict(sess, next_states_batch)\n",
    "            targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * discount_factor * np.amax(q_values_next, axis=1)\n",
    "            \n",
    "            states_batch = np.array(states_batch)\n",
    "            loss = q_estimator.update(sess, states_batch, action_batch, targets_batch)\n",
    "            \n",
    "            if done:\n",
    "                round_num.append(t)\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "            total_t += 1\n",
    "            \n",
    "tf.reset_default_graph()\n",
    "\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "q_estimator = Estimator(scope=\"q_estimator\")\n",
    "target_estimator = Estimator(scope=\"target_q\")\n",
    "\n",
    "state_processor = StateProcessor()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    deep_q_learning(sess,\n",
    "                    env,\n",
    "                    q_estimator=q_estimator,\n",
    "                    target_estimator=target_estimator,\n",
    "                    state_processor=state_processor,\n",
    "                    num_episodes=10000,\n",
    "                    replay_memory_size=500000,\n",
    "                    replay_memory_init_size=50000,\n",
    "                    update_target_estimator_every=10000,\n",
    "                    epsilon_start=1.0,\n",
    "                    epsilon_end=0.1,\n",
    "                    epsilon_decay_steps=500000,\n",
    "                    discount_factor=0.99,\n",
    "                    batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-python3",
   "language": "python",
   "name": "jupyter-python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
